\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}

\title{Multilingual Evaluation of a Retrieval-Augmented Generation System for Admissions Question Answering}
\author{Abdul Mannaan Sayyad}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This study presents an evaluation of a multilingual Retrieval-Augmented Generation (RAG) system designed to answer questions related to Master's admissions in France. Using curated bilingual evaluation datasets, we assess retrieval quality and grounding faithfulness across English and French queries. The system achieves 90\% factual faithfulness in both languages and demonstrates consistent recall at higher ranks, with notably improved top-1 retrieval performance for French queries. Our results indicate the viability of RAG pipelines for reliable, multilingual academic decision support.
\end{abstract}

\section{Introduction}

Large language models (LLMs) enable flexible question answering across domains but often suffer from hallucinations when operating without verified grounding sources. Retrieval-Augmented Generation (RAG) mitigates this risk by combining semantic document retrieval with controlled generation constrained to retrieved context. In high-stakes informational domains such as academic admissions, reliability and factual consistency are critical.

This work evaluates a domain-specific RAG pipeline applied to French higher education admissions. We focus on bilingual robustness, testing system stability across English and French queries with controlled evaluation datasets derived from official admissions publications.

\section{System Architecture}

The pipeline consists of three core components:

\begin{itemize}
    \item \textbf{Embedding Model:} Sentence-Transformers paraphrase-multilingual-mpnet-base-v2 for cross-lingual encoding.
    \item \textbf{Retrieval Engine:} FAISS vector indexing over chunked source documents.
    \item \textbf{Generation Model:} Phi-3 Mini language model operating under strict prompt constraints to ensure context-grounded responses.
\end{itemize}

Incoming user queries are embedded and matched against the vector index. Retrieved passages are injected into a structured prompt that enforces excerpt-constrained response generation. This design minimizes unsupported speculation and improves factual grounding.

\section{Dataset Construction}

Evaluation was conducted using two manually curated datasets: 10 English questions and 10 exact French counterparts. All queries reference information contained within five official public sources from Campus France, Universit√© Paris-Saclay, Grenoble AI4OneHealth, and HEC Paris documentation.

Each question is assigned a single gold-source document for retrieval verification. The datasets and outputs are publicly released to facilitate reproducibility and benchmarking.

\section{Evaluation Protocol}

For each query, the system retrieves the top-5 ranked passages. We measure:

\begin{itemize}
    \item \textbf{Faithfulness Accuracy:} Whether the correct gold document appears in the retrieved set.
    \item \textbf{Recall@k:} Retrieval success at ranks $k\in\{1,3,5\}$.
\end{itemize}

Answer outputs are manually inspected to confirm that generated responses remain faithful to retrieved context without introducing unsupported claims.

\section{Results}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Language & Faithfulness & Recall@1 & Recall@3 & Recall@5\\
\midrule
English & 90\% & 50\% & 90\% & 90\% \\
French  & 90\% & 70\% & 90\% & 90\% \\
\bottomrule
\end{tabular}
\caption{Multilingual retrieval and grounding performance}
\end{table}

Results indicate strong system stability across languages, with identical faithfulness and high recall at broader candidate retrieval levels. Higher Recall@1 on French queries likely reflects greater lexical alignment between source documents and user inputs.

\section{Error Analysis}

Only one failure case was observed, attributable to semantic overlap between general Campus France documentation and more specialized admissions guides, leading to misranking of the relevant source. This highlights limitations inherent to dense retrieval methods when documents share substantial topical similarity. Future work will explore cross-encoder reranking models to refine top-1 retrieval precision.

\section{Discussion and Limitations}

Although evaluation results are promising, dataset scale remains limited. Expanding question diversity and incorporating adversarial hallucination stress tests would further validate system reliability. Additionally, larger bilingual sample sizes are necessary to confirm observed top-rank retrieval trends.

\section{Conclusion}

This study demonstrates the effectiveness of a multilingual RAG pipeline for specialized educational QA tasks, achieving reliable grounding across language boundaries. Public release of the full system, datasets, and evaluation notebooks enables transparent replication and future comparative research.

\section*{Reproducibility}

All source code, evaluation scripts, and the deployed RAG system are publicly available:

\begin{itemize}
    \item GitHub repository: \url{https://github.com/abdulmannaan502/french-masters-rag-advisor}
    \item Live system demo: \url{https://huggingface.co/spaces/abdulmannaan1/ai-masters-advisor-france}
    \item Evaluation dataset: \url{https://www.kaggle.com/datasets/abdulmannaan12/french-masters-rag-eval}
    \item Reproducible notebook: \url{https://www.kaggle.com/code/abdulmannaan12/multilingual-rag-evaluation-for-french-admissions}
\end{itemize}

\end{document}
